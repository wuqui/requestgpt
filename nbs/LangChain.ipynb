{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requestgpt.src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    49617\n",
       "1      383\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(fp='../in/utterances_requests_50k_labeled.csv')\n",
    "\n",
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use smaller DF: 50/50 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    383\n",
       "1    383\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_requests = len(df.query('label == 1'))\n",
    "\n",
    "df = (df\n",
    " .sort_values('label', ascending=False)\n",
    " .head(n_requests*2)\n",
    ")\n",
    "\n",
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    "    create_openai_fn_runnable,\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class UtteranceClassification(BaseModel):\n",
    "    \"\"\"Identifying whether the utterance is a request or not.\"\"\"\n",
    "    gpt_label: int = Field(..., description=\"Whether this utterance is a request (0) or not a request (1)\")\n",
    "    gpt_justification: str = Field(..., description=\"Why you decided that it is a request or not a request.\")\n",
    "    gpt_confidence: int = Field(..., description=\"How confident you (ChatGPT) are in your decision, on an ordinal scale from 1 (very inconfident) to 5 (very confident)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "\tmodel=\"gpt-3.5-turbo\", \n",
    "\t# model=\"gpt-4\", \n",
    "    temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", read_prompt('../in/prompts/prompt_task.md')),\n",
    "        (\"system\", read_prompt('../in/prompts/prompt_few-shot-examples.md')),\n",
    "        (\"system\", read_prompt('../in/prompts/prompt_chain-of-thought.md')),\n",
    "        (\"human\", 'Please classify the following utterance: {input}')]\n",
    ")\n",
    "\n",
    "runnable = create_structured_output_runnable(\n",
    "    UtteranceClassification, llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_all = load_data()\n",
    "utterances_done = pd.read_csv('../out/results_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_todo = utterances_all.merge(utterances_done, on=['text_id', 'u_n'], how='left', indicator=True)\n",
    "utterances_todo = utterances_todo[utterances_todo['_merge'] == 'left_only']\n",
    "utterances_todo = utterances_todo.drop(columns=[col for col in utterances_todo.columns if col.endswith('_y') or col == '_merge'])\n",
    "utterances_todo.columns = [col.replace('_x', '') for col in utterances_todo.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>done</td>\n",
       "      <td>49160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0   all  50000\n",
       "1  done  49160\n",
       "2  todo    840"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=[\n",
    "\t['all', len(utterances_all)],\n",
    "\t['done', len(utterances_done)],\n",
    "\t['todo', len(utterances_all) - len(utterances_done)]\t\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for _, row in tqdm(utterances_todo.iterrows(), total=len(utterances_todo)):\n",
    "    result = runnable.invoke({\"input\": row['text']})\n",
    "    row_data = {\n",
    "        \"text_id\": row['text_id'],\n",
    "        \"u_n\": row['u_n'],\n",
    "        \"u_who\": row['u_who'],\n",
    "        \"text\": row['text'],\n",
    "        \"label\": row['label'],\n",
    "        \"gpt_label\": result.gpt_label,\n",
    "        \"gpt_justification\": result.gpt_justification,\n",
    "        \"gpt_confidence\": result.gpt_confidence       \n",
    "    }\n",
    "    results_list.append(row_data)\n",
    "    pd.DataFrame([row_data]).to_csv('../out/results_temp.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../out/langchain_gpt3.5_766.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker is prompting the listener to provide another Coke, which brings advantages for the speaker.\n",
      "The speaker is prompting the listener to put something on the plates, which implies a future action. This action could bring advantages for the speaker, such as convenience or organization.\n",
      "The utterance 'and stuff' does not prompt the listener to a future action and does not bring advantages for the speaker. Therefore, it is not a request.\n",
      "The utterance does not prompt the listener to a future action or bring advantages for the speaker.\n",
      "The speaker is prompting the listener to provide something ('what') in the future.\n"
     ]
    }
   ],
   "source": [
    "for _, row in results_df.sample(5).iterrows():\n",
    "\tprint(row['gpt_justification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_txt = ''\n",
    "\n",
    "for i in range(len(prompt.messages)):\n",
    "\tprompt_txt += prompt.messages[i].prompt.template + '\\n'\n",
    "\n",
    "display(Markdown(prompt_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(fname, model):\n",
    "\tdf = pd.read_csv(fname)\n",
    "\tdf = df.dropna()\n",
    "\tprecision, recall, f1, _ = precision_recall_fscore_support(df[\"label\"], df[\"gpt_label\"], average=\"macro\")\n",
    "\tmetrics = pd.DataFrame({\n",
    "\t\t'model': model,\n",
    "\t\t'precision': precision,\n",
    "\t\t'recall': recall,\n",
    "\t\t'f1': f1\n",
    "\t}, index=[0])\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_gpt = get_metrics('../out/langchain_gpt3.5_766.csv', 'gpt3.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_bert = pd.DataFrame({'model': 'BERT', 'precision': 0.21, 'recall': 0.43, 'f1': 0.28}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.concat([metrics_bert, metrics_gpt])\n",
    "metrics = metrics.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"score\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results with altair \n",
    "import altair as alt\n",
    "\n",
    "bars = alt.Chart(metrics).mark_bar().encode(\n",
    "    y=alt.Y(\"score\", title=''),\n",
    "    x=alt.X(\"model\", title=''),\n",
    "    color=\"model\",\n",
    ")\n",
    "\n",
    "text = alt.Chart(metrics).mark_text(dx=0, dy=-5, color='blue').encode(\n",
    "    y=\"score\",\n",
    "    x=\"model\",\n",
    "    detail='score',\n",
    "    text=alt.Text('score', format='.2f'),\n",
    "    color='model'\n",
    ")\n",
    "\n",
    "chart = alt.layer(bars, text).facet(\n",
    "    column=alt.Column(\"metric\", sort=[\"precision\", \"recall\", \"f1\"])\n",
    ").resolve_scale(y='independent')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/requestgpt/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# chart.save('../out/metrics_langchain_gpt3.5.png', scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>u_n</th>\n",
       "      <th>u_who</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>gpt_label</th>\n",
       "      <th>gpt_justification</th>\n",
       "      <th>gpt_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S6BR</td>\n",
       "      <td>401</td>\n",
       "      <td>S0474</td>\n",
       "      <td>just press unlock first and then up yeah hold ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The speaker is giving instructions, but there ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S632</td>\n",
       "      <td>5575</td>\n",
       "      <td>S0220</td>\n",
       "      <td>you keep that shit away from my shit though</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This utterance is not a request because the sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SAZX</td>\n",
       "      <td>1895</td>\n",
       "      <td>S0600</td>\n",
       "      <td>pardon?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The utterance 'pardon?' is not a request becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S632</td>\n",
       "      <td>3087</td>\n",
       "      <td>S0202</td>\n",
       "      <td>please don't</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The utterance does not prompt the listener to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S632</td>\n",
       "      <td>4983</td>\n",
       "      <td>S0202</td>\n",
       "      <td>let me see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The utterance 'let me see' is not a request be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>SYHP</td>\n",
       "      <td>2781</td>\n",
       "      <td>S0261</td>\n",
       "      <td>has it got er cinnamon in it?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The speaker is asking if the item has cinnamon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>SDS7</td>\n",
       "      <td>53</td>\n",
       "      <td>S0439</td>\n",
       "      <td>oh okay so what do we have going on? we have n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The speaker is prompting the listener to provi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>SY5K</td>\n",
       "      <td>598</td>\n",
       "      <td>S0651</td>\n",
       "      <td>actually some government ought to come in and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The speaker is suggesting that a government sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>S6W8</td>\n",
       "      <td>491</td>\n",
       "      <td>S0492</td>\n",
       "      <td>you got her?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The speaker is asking if the listener has some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>SR5J</td>\n",
       "      <td>1622</td>\n",
       "      <td>S0493</td>\n",
       "      <td>go on</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The utterance 'go on' is a request because the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id   u_n  u_who                                               text  \\\n",
       "1      S6BR   401  S0474  just press unlock first and then up yeah hold ...   \n",
       "6      S632  5575  S0220        you keep that shit away from my shit though   \n",
       "14     SAZX  1895  S0600                                            pardon?   \n",
       "15     S632  3087  S0202                                      please don't    \n",
       "16     S632  4983  S0202                                         let me see   \n",
       "..      ...   ...    ...                                                ...   \n",
       "678    SYHP  2781  S0261                      has it got er cinnamon in it?   \n",
       "680    SDS7    53  S0439  oh okay so what do we have going on? we have n...   \n",
       "711    SY5K   598  S0651  actually some government ought to come in and ...   \n",
       "722    S6W8   491  S0492                                       you got her?   \n",
       "764    SR5J  1622  S0493                                             go on    \n",
       "\n",
       "     label  gpt_label                                  gpt_justification  \\\n",
       "1        1          0  The speaker is giving instructions, but there ...   \n",
       "6        1          0  This utterance is not a request because the sp...   \n",
       "14       1          0  The utterance 'pardon?' is not a request becau...   \n",
       "15       1          0  The utterance does not prompt the listener to ...   \n",
       "16       1          0  The utterance 'let me see' is not a request be...   \n",
       "..     ...        ...                                                ...   \n",
       "678      0          1  The speaker is asking if the item has cinnamon...   \n",
       "680      0          1  The speaker is prompting the listener to provi...   \n",
       "711      0          1  The speaker is suggesting that a government sh...   \n",
       "722      0          1  The speaker is asking if the listener has some...   \n",
       "764      0          1  The utterance 'go on' is a request because the...   \n",
       "\n",
       "     gpt_confidence  \n",
       "1                 0  \n",
       "6                 0  \n",
       "14                0  \n",
       "15                1  \n",
       "16                0  \n",
       "..              ...  \n",
       "678               0  \n",
       "680               0  \n",
       "711               0  \n",
       "722               0  \n",
       "764               0  \n",
       "\n",
       "[116 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatches = results_df.query('label != gpt_label')\n",
    "mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches.to_csv('../out/mismatches.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requestgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
